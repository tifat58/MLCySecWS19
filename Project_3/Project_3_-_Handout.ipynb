{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4NzyqehHjUlG"
   },
   "source": [
    "# ML in Cybersecurity: Project III\n",
    "\n",
    "## Team\n",
    "  * **Team name**:  *fill this in*\n",
    "  * **Members**:  *fill this in. format: name1 (email1), name2 (email2), ...*\n",
    "  * **Tutor**: *fill this in*\n",
    "\n",
    "\n",
    "## Logistics\n",
    "  * **Due date**: 12th December 2019, 13:59:59 \n",
    "  * Email the completed notebook to mlcysec_ws1920_staff@lists.cispa.saarland \n",
    "  * Complete this in the previously established **teams of 3**\n",
    "  * Feel free to use the course [mailing list](https://lists.cispa.saarland/listinfo/mlcysec_ws1920_stud) to discuss.\n",
    "  \n",
    "## Timeline\n",
    "  * 28-Nov-2019: Project 3 hand-out\n",
    "  * **12-Dec-2019** (13:59:59): Email completed notebook to mlcysec_ws1920_staff@lists.cispa.saarland\n",
    "\n",
    "  * 19-Dec-2019: Project 3 discussion and summary\n",
    "  \n",
    "  \n",
    "## About this Project\n",
    "In this project, we dive into the vulnerabilities of machine learning models and the difficulties of defending against them. To this end, we require you to implement an evasion attack (craft adversarial examples) yourselves, and defend your own model.   \n",
    "\n",
    "\n",
    "## A Note on Grading\n",
    "The total number of points in this project is 100. We further provide the number of points achievable with each excercise. You should take particular care to document and visualize your results, though.\n",
    "\n",
    "\n",
    " \n",
    "## Filling-in the Notebook\n",
    "You'll be submitting this very notebook that is filled-in with (all!) your code and analysis. Make sure you submit one that has been previously executed in-order. (So that results/graphs are already visible upon opening it). \n",
    "\n",
    "The notebook you submit **should compile** (or should be self-contained and sufficiently commented). Check tutorial 1 on how to set up the Python3 environment.\n",
    "\n",
    "It is extremely important that you **do not** re-order the existing sections. Apart from that, the code blocks that you need to fill-in are given by:\n",
    "```\n",
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "#\n",
    "```\n",
    "Feel free to break this into multiple-cells. It's even better if you interleave explanations and code-blocks so that the entire notebook forms a readable \"story\".\n",
    "\n",
    "\n",
    "## Code of Honor\n",
    "We encourage discussing ideas and concepts with other students to help you learn and better understand the course content. However, the work you submit and present **must be original** and demonstrate your effort in solving the presented problems. **We will not tolerate** blatantly using existing solutions (such as from the internet), improper collaboration (e.g., sharing code or experimental data between groups) and plagiarism. If the honor code is not met, no points will be awarded.\n",
    "\n",
    " \n",
    " ## Versions\n",
    "  * v1.0: Initial notebook\n",
    "  * v1.1: Clarifications at 1.1.2, 1.2.2, 2.1\n",
    " \n",
    "  ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ewNwfFvbFaR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    " \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import json \n",
    "import time \n",
    "import pickle \n",
    "import sys \n",
    "import csv \n",
    "import os \n",
    "import os.path as osp \n",
    "import shutil \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display, HTML\n",
    " \n",
    "%matplotlib inline \n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots \n",
    "plt.rcParams['image.interpolation'] = 'nearest' \n",
    "plt.rcParams['image.cmap'] = 'gray' \n",
    " \n",
    "# for auto-reloading external modules \n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "640GrzbOevr0"
   },
   "outputs": [],
   "source": [
    "# Some suggestions of our libraries that might be helpful for this project\n",
    "from collections import Counter          # an even easier way to count\n",
    "from multiprocessing import Pool         # for multiprocessing\n",
    "from tqdm import tqdm                    # fancy progress bars\n",
    "\n",
    "# Load other libraries here.\n",
    "# Keep it minimal! We should be easily able to reproduce your code.\n",
    "# We only support sklearn and pytorch.\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "\n",
    "# We preload pytorch as an example\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GJZPEAWYMhYB"
   },
   "outputs": [],
   "source": [
    "compute_mode = 'cpu'\n",
    "\n",
    "if compute_mode == 'cpu':\n",
    "    device = torch.device('cpu')\n",
    "elif compute_mode == 'gpu':\n",
    "    # If you are using pytorch on the GPU cluster, you have to manually specify which GPU device to use\n",
    "    # It is extremely important that you *do not* spawn multi-GPU jobs.\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'    # Set device ID here\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    raise ValueError('Unrecognized compute mode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nxi-lLD0mKHD"
   },
   "source": [
    "#### Helpers\n",
    "\n",
    "In case you choose to have some methods you plan to reuse during the notebook, define them here. This will avoid clutter and keep rest of the notebook succinct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VBbigqdEmKd8"
   },
   "outputs": [],
   "source": [
    "def identity_func(foo):\n",
    "    return foo\n",
    "\n",
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n1pcmKkyjT7y"
   },
   "source": [
    "# 1. Attacking an ML-model\n",
    "\n",
    "In this section, we implement an attack ourselves. We then leverage the Foolbox library to craft adversarial examples. First, however, you need a model you can attack. Feel free to choose the DNN/ConvNN from project 1.\n",
    "\n",
    "Hint: you might want to save the trained model to save time later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QaJv_d_Dp7OM"
   },
   "source": [
    "### 1.1.1: Setting up the model (5 Points)\n",
    "\n",
    "Re-use the model from project 1 here and train it until it achieves reasonable accuracy (>92%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c688qdGtO1v-"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epoch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6dbe72d8ca72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# (4)train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# (5)evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch %d, Train acc: %f, Test acc: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'epoch' is not defined"
     ]
    }
   ],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "#\n",
    "\n",
    "# (1)load data \n",
    "# (2)define model\n",
    "# (3)define loss, optimizer \n",
    "# (4)train\n",
    "# (5)evaluate\n",
    "\n",
    "\n",
    "print('Epoch %d, Train acc: %f, Test acc: %f' % (epoch, train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 64 \n",
    "batch_size_test = 1000 \n",
    "n_epochs = 12\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('data/mnist/', train=True, download=True,\n",
    "                              transform=torchvision.transforms.Compose([\n",
    "                                  torchvision.transforms.ToTensor()\n",
    "                              ])), batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('data/mnist/', train=False, download=True,\n",
    "                              transform=torchvision.transforms.Compose([\n",
    "                                  torchvision.transforms.ToTensor(),\n",
    "                              ])), batch_size=batch_size_test, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self,kernels=5):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 10 output channels, 5x5 square convolution\n",
    "        # kernel \n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=kernels)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=kernels)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs):\n",
    "    net.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = net(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset), \n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "            train_counter.append(\n",
    "                (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "            \n",
    "            torch.save(net.state_dict(), 'model_3.pth')\n",
    "    train_losses.append(loss.item()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    net.eval()\n",
    "    test_loss = 0 \n",
    "    correct = 0 \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = net(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "            \n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        test_losses.append(test_loss)\n",
    "        print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, correct, len(test_loader.dataset), \n",
    "            100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.313071\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.273833\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.117910\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.467596\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.154780\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.100626\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.903262\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.760834\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.779079\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.684896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.3343, Accuracy: 9082/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.529601\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.616678\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.538682\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.621214\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.453160\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.245588\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.401746\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.453412\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.395740\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.556283\n",
      "\n",
      "Test set: Avg. loss: 0.1999, Accuracy: 9385/10000 (93%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.322680\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.756606\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.601465\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.389893\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.235579\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.438259\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.310439\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.534584\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.310173\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.407661\n",
      "\n",
      "Test set: Avg. loss: 0.1530, Accuracy: 9537/10000 (95%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.275381\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.405190\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.356368\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.453632\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.244171\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.421449\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.272772\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.231889\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.350508\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.341752\n",
      "\n",
      "Test set: Avg. loss: 0.1228, Accuracy: 9645/10000 (96%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.369237\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.355701\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.295688\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.217609\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.199643\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.178119\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.392890\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.333231\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.253754\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.245005\n",
      "\n",
      "Test set: Avg. loss: 0.1101, Accuracy: 9663/10000 (96%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.357022\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.388829\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.204642\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.239683\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.488484\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.338140\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.174452\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.460514\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.705890\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.271243\n",
      "\n",
      "Test set: Avg. loss: 0.0981, Accuracy: 9706/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.201130\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.340242\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.259685\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.270641\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.506955\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.401278\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.216840\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.317460\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.227231\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.092389\n",
      "\n",
      "Test set: Avg. loss: 0.0855, Accuracy: 9735/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.137605\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.196091\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.324494\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.667567\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.129227\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.331936\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.158852\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.229731\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.140586\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.231703\n",
      "\n",
      "Test set: Avg. loss: 0.0791, Accuracy: 9744/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.217013\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.400796\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.293358\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.264883\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.165096\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.085911\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.206721\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.233672\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.130476\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.141964\n",
      "\n",
      "Test set: Avg. loss: 0.0761, Accuracy: 9761/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.219469\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.191309\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.183206\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.235636\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.130003\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.155454\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.219182\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.262924\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.373434\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.227272\n",
      "\n",
      "Test set: Avg. loss: 0.0722, Accuracy: 9770/10000 (97%)\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.160124\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 0.171320\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.278185\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.135385\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.127769\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.371990\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.173255\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 0.173986\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.166915\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.174029\n",
      "\n",
      "Test set: Avg. loss: 0.0683, Accuracy: 9783/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
    "net = Net()\n",
    "optimizer = optim.SGD(net.parameters(), lr= learning_rate, momentum=momentum)\n",
    "\n",
    "for epoch in range(1, n_epochs):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DEQrdyLHsUIu"
   },
   "source": [
    "### 1.1.2: Implementing an attack (15 Points)\n",
    "\n",
    "We now want you to attack the model trained in the previous step. Please implement the FGSM attack mentioned in the lecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gcVZnUNbRKOz"
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "#\n",
    "def FGSM(img, delt, gradient):\n",
    "    grad_val = gradient.sign()\n",
    "    purturbed_img = img + delt * grad_val\n",
    "    return torch.clamp(purturbed_img,0,1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_purturbed_image(model, data_loader, deta, device):\n",
    "    noisy_example_correct = []\n",
    "    noise_example_wrong = []\n",
    "    correct_example = 0\n",
    "    \n",
    "    for img, target in data_loader:\n",
    "        img, target = img.to(device), target.to(device)\n",
    "        img.requires_grad = True\n",
    "        out = model(img)\n",
    "        initial_prediction = out.max(1,keepdim=True)[1]\n",
    "        if initial_prediction.item() != target.item():\n",
    "            # Skip the iteration if the prediction is correct\n",
    "            continue \n",
    "        \n",
    "        loss = F.nll_loss(out, target)\n",
    "        model.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        gradient_img = img.grad.data\n",
    "        purturbed_img = FGSM(img, delta, gradient_img)\n",
    "        out = model(purturbed_img)\n",
    "        purturbed_out = out.max(1,keepdim=True)[1]\n",
    "        if purturbed_out.item() == target.item():\n",
    "            correct_example += 1\n",
    "            if(len(noisy_example_correct)< 5):\n",
    "                adv_example = purturbed_img.squeeze().detach().cpu().numpy()\n",
    "                noisy_example_correct.append( (adv_example) )\n",
    "        else :\n",
    "            if len(noise_example_wrong) < 5 : \n",
    "                adv_example = purturbed_img.squeeze().detach().cpu().numpy()\n",
    "                noisy_example_correct.append( (adv_example) )\n",
    "        accuray = correct_example /float(len(data_loader)) \n",
    "        print('Accuracy : ', accuray, 'delta : ', delta )\n",
    "        return accuray , noise_example_wrong, noisy_example_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "model.load_state_dict(torch.load('model_3.pth'),strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.0001 delta :  0.01\n",
      "Accuracy :  0.0001 delta :  0.05\n",
      "Accuracy :  0.0001 delta :  0.15\n",
      "Accuracy :  0.0001 delta :  0.2\n",
      "Accuracy :  0.0 delta :  0.25\n",
      "Accuracy :  0.0 delta :  0.3\n",
      "Accuracy :  0.0 delta :  0.35\n",
      "Accuracy :  0.0 delta :  0.4\n",
      "Accuracy :  0.0 delta :  0.6\n",
      "[0.0001, 0.0001, 0.0001, 0.0001, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mak/python3-venv/mlcysec/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "deltas = [0.01, 0.05, 0.15, 0.2, 0.25,0.3,0.35, 0.4,0.6]\n",
    "\n",
    "\n",
    "accurcay = []\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('data/mnist/', train=False, download=True,\n",
    "                              transform=torchvision.transforms.Compose([\n",
    "                                  torchvision.transforms.ToTensor(),\n",
    "                              ])), batch_size=1, shuffle=True)\n",
    "\n",
    "for delta in deltas:\n",
    "    accu, b, g =  predict_purturbed_image(model, test_loader, delta, device)\n",
    "    accurcay.append(accu)\n",
    "    \n",
    "    \n",
    "print (accurcay)\n",
    "\n",
    "# choose best delta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(deltas, accurcay, \"*-\")\n",
    "plt.yticks(np.arange(0, 1.1, step=0.1))\n",
    "plt.xticks(np.arange(0, .35, step=0.05))\n",
    "plt.title(\"Accuracy vs Deltas\")\n",
    "plt.xlabel(\"Deltas\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RNpI3oUoO1wE"
   },
   "source": [
    "### 1.1.3: adversarial sample set (5 Points)\n",
    "\n",
    "Please additionally generate a dataset containing at least 1,000 adversarial examples using FGSM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EvYpo9p2O1wF"
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ex3qQp3JolD1"
   },
   "source": [
    "### 1.1.3: Visualizing the results (5 Points)\n",
    "\n",
    "Please chose one sample for each class (for example the first when iterating the test data) and plot the (ten) adversarial examples as well as the predicted label (before and after the attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eGkp0B0PO1wJ"
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iPB-GK1CymiV"
   },
   "source": [
    "### 1.2.1: Using libraries for attacks (10 Points)\n",
    "As the field of evasion attacks (in particular for DNN) is very active research field, several libraries have been published that contain attacks. We will work here with the Foolbox (https://github.com/bethgelab/foolbox) library. Please implement two other (recent, advanced) attacks of your choice using this library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pa6rPT53LUW8"
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "#\n",
    "\n",
    "# (a) attack 1\n",
    "# (b) attack 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xVH821TOymic"
   },
   "source": [
    "### 1.2.2: Visualizing the results (20 Points)\n",
    "As before, please plot the new adversarial examples. Compare all crafting techniques (FGSM, 2 methods from Foolbox).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "69nc8PMRymid"
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "#\n",
    "\n",
    "# template code (Please feel free to change this)\n",
    "# (each column corresponds to one attack method)\n",
    "col_titles = ['Ori','FGSM','Method 1', 'Method 2'] \n",
    "nsamples = 10\n",
    "nrows = nsamples\n",
    "ncols = len(col_titles)\n",
    "\n",
    "fig, axes = plt.subplots(nrows,ncols,figsize=(8,12))  # create the figure with subplots\n",
    "[ax.set_axis_off() for ax in axes.ravel()]  # remove the axis\n",
    "\n",
    "for ax, col in zip(axes[0], col_titles): # set up the title for each column\n",
    "    ax.set_title(col,fontdict={'fontsize':18,'color':'b'})\n",
    "\n",
    "for i in range(nsamples):\n",
    "    axes[i,0].imshow(images_ori[i])\n",
    "    axes[i,1].imshow(adv_FGSM[i])\n",
    "    axes[i,2].imshow(adv_Method1[i])\n",
    "    axes[i,3].imshow(adv_Method2[i])\n",
    "                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m6HxYLl3O1wV"
   },
   "source": [
    "Which differences do you observe when comparing different attack methods? Why?   \n",
    "Please write a brief summary of your findings.   \n",
    "* Does the attack always succeed (the model make wrong prediction on the adversarial sample)?\n",
    "* How different is the adversarial sample from the original image?\n",
    "(L0,L2,Linf norm)  \n",
    "* How about the computation cost of each attack method?\n",
    "* Does the attack require white-box access to the model?\n",
    "* ....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KJUmrv5Bymij"
   },
   "source": [
    "# 2. Defending an ML model\n",
    "\n",
    "So far, we have focused on attacking an ML model. In this section, we want you to defend your model. As before concerning the attack, you can chose an example from the lecture, or experiment with any idea you have.\n",
    "\n",
    "We do not require the defense to work perfectly - but what we want you to understand is why it works or why it does not work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0gHUFK6Mymik"
   },
   "source": [
    "### 2.1: Implementing a defense of your choice (25 Points)\n",
    "As stated before, feel free to implement a defense or mitigation of your choice. Evaluate the defense on adversarial examples. This entails at least the 1,000 examples crafted from FGSM.   \n",
    "Also, you are encouraged (optional) to defend against the two other attack methods, i.e. you are free to increase this special test set (for example by >30 examples (>10 from your FGSM attack, >10 from both the two other attacks of the library))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DD0UalSeymim"
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "#\n",
    "print('Accuracy on adversarial samples (FGSM) %.2f'%acc_FGSM)\n",
    "print('Accuracy on adversarial samples (FGSM) after defense %.2f'%acc_FGSM_defend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sIMRqecuymir"
   },
   "source": [
    "### 2.2: Conclusions (15 Points)\n",
    "Please interpret the results of your defense here. \n",
    "\n",
    "* What did you try to make the classifier more robust against FGSM? \n",
    "* Why did it work? \n",
    "* Is the classifier now robust against FGSM?  \n",
    "* ...\n",
    "\n",
    "Feel free to state any interesting finding you encountered during this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "scROCZjDymit"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Project_3_Template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
